## **System Overview**

```
📷 Camera / Stream (/dev/video0)
       │
       ▼
🧩 capture_frames.py → /opt/missions/poses.json
       │
       │ Sends image_path requests to
       ▼
🌐 main_with_time_and_json_and_image_http.py (VILA API Server)
       │
       │ Forwards results to
       ▼
🖥️ receiver_from_vila_with_image.py (Collector)
       └── ./ingested/  ←  JSON + Images stored here
```

This creates a full real-time pipeline from **camera capture** → **VLM description** → **remote collection**, ready for robotics, AI perception, or dataset generation workflows.



### 1. main_with_time_and_json_and_image_http.py

**Purpose:**
This script runs the **VILA model** as an HTTP API server that receives image paths from external clients, generates a textual description for each image, saves the results to a per-image JSON locally, and forwards both the JSON and the source image to a remote collector over HTTP (default: `http://172.16.17.11:5000/ingest`).
On the collector side, the files are saved under `./ingested` as `<basename>.json` and `<basename>.jpg` (or the original image extension if not JPG).

---

#### **Container Setup (Jetson)**

Run the container that hosts the VILA model:

```bash
jetson-containers run -it \
  --publish 8080:8080 \
  --volume /home/user/jetson-containers/data:/mnt/VLM/jetson-data \
  nano_llm_custom /bin/bash
```

🧠 **Note:**
`jetson-containers` automatically mounts `/home/user/jetson-containers/data` to `/data` inside the container.
The extra `--volume` mount ensures that symbolic links resolving to `/mnt/VLM/jetson-data` are also accessible inside the container (important when using NVMe storage).

---


---

#### 🚀 **Running the VILA Server**

Start the HTTP server that exposes the model API:

```bash
python3 -m nano_llm.chat \
  --api=mlc \
  --model Efficient-Large-Model/VILA1.5-3b \
  --max-context-len 256 \
  --max-new-tokens 32 \
  --save-json-by-image \
  --server --port 8080
```

Once running, the server will listen at:

```
http://<JETSON_IP>:8080
```

---

#### **Testing the API**

##### Health Check

```bash
curl http://172.16.17.12:8080/health
```

##### Describe an Image

```bash
curl -X POST http://172.16.17.12:8080/describe \
  -H "Content-Type: application/json" \
  -d '{"image_path": "/data/images/01.jpg"}'
```

##### 💬 Ask a Follow-up Question

```bash
curl -X POST http://172.16.17.12:8080/describe \
  -H "Content-Type: application/json" \
  -d '{"image_path": "/data/images/01.jpg", "question": "what is the color of the drone"}'
```

---

### 2. capture_frames.py

**Purpose:**
This script captures frames from a **live video stream** (e.g., `/dev/video0`, RTSP, or file) and saves images every few seconds or based on a predefined set of **poses** listed in `/opt/missions/poses.json`.
Each captured frame is saved with a matching `.json` file containing both the 3D pose and an optional description generated by the VLM model.
The script can automatically call the VLM HTTP endpoint (e.g., `http://172.16.17.12:8080/describe`) for every captured frame.

---

#### **Pose File Example**

```json
[
  { "x": -0.200, "y":  0.000, "z": 0.300, "yaw": 0.000000 },
  { "x":  0.000, "y": -0.200, "z": 0.300, "yaw": 1.570796 },
  { "x":  0.200, "y":  0.000, "z": 0.300, "yaw": 3.141593 },
  { "x":  0.000, "y":  0.200, "z": 0.300, "yaw": 4.712389 }
]
```

Each entry defines a position (`x`, `y`, `z`) and orientation (`yaw` in radians).       └── ./ingested/  ←  JSON + Images stored here
The script iterates through these poses, capturing one image per pose, and saves the data accordingly.

---

#### **How to Run**

Run the script from inside the images directory:

```bash
cd /home/user/jetson-containers/data/images
python3 capture_frames.py --source /dev/video0 --vlm http://172.16.17.12:8080/describe
```

This will:

1. Open the live camera feed from `/dev/video0`.
2. Load the list of poses from `/opt/missions/poses.json`.
3. Capture one frame per pose every few seconds.
4. Save the images and JSON files (including the VLM-generated captions) under a timestamped folder inside `captures/`.

* if the camera change input you can check
```bash
v4l2-ctl --list-devices
```
and change to /dev/video1 for example.

---

#### **Output Structure**

```
captures/
  2025_10_09___13_45_22/
    x0200y0000z0300yaw0000000.jpg
    x0200y0000z0300yaw0000000.json
    x0000y0200z0300yaw1570796.jpg
    x0000y0200z0300yaw1570796.json
    ...
```

Each pair of `.jpg` and `.json` files contains an image and its corresponding metadata (pose + VLM caption).

---

#### 🎮 **Interactive Mode (Manual Capture)**

in folder  
```bash
/home/user/jetson-containers/data/images
```

```bash
python3 capture_frames.py --source /dev/video0 --interactive --preview --vlm http://172.16.17.12:8080/describe
```

In this mode:

* Press **SPACE** or **C** to capture a frame.
* Press **Q** to quit.

The captured images and metadata are automatically saved, including optional VLM captions.

---

### 3. Display Server (Web GUI Viewer)

**Purpose:**
The Display Server provides a real-time graphical interface (GUI) to view all collected images and their textual descriptions.
It scans the captures/ or ingested/ directories, displays each image as a thumbnail, and shows the description text extracted from the corresponding JSON file.
This helps to monitor VILA outputs, verify capture quality, and inspect dataset generation visually.

**How It Works:**

```bash
📁 ./captures/<timestamp>/
   ├── image_01.jpg
   ├── image_01.json  ← contains VILA caption
   ├── image_02.jpg
   ├── image_02.json
   └── ...

🌐 display_server.py
   └── launches a local web server (Flask) to visualize all image–JSON pairs
```
The server automatically scans all sub-folders under the root directory you provide (--root) and refreshes the grid every few seconds.

in folder:
```bash
/home/user/shir
```

**Installation:**
```bash
pip install flask
```

**Running the Display Server**
```bash
python3 display_server.py \
  --root /home/user/jetson-containers/data/images/captures \
  --host 0.0.0.0 \
  --port 8090
```
Adjust the --root path if your captures are stored elsewhere
(e.g. /home/user/ingested for the remote collector machine).

**Accessing the Web Interface**
Open your browser and navigate to:
```bash
http://<DEVICE_IP>:8090
```
Example:

```bash
http://172.16.17.12:8090
```

You’ll see a dark-themed dashboard with all your recent captures displayed in a responsive grid view.
The interface auto-refreshes every 2 seconds — you can also press “Refresh now” at the top-right corner to reload manually.


### 4. Receiver (Remote Collector on 172.16.17.11)

Run a lightweight Flask service that receives the JSON + image files from the Jetson device and saves them locally:

```bash
# on 172.16.17.11
python3 -m venv venv && source venv/bin/activate
pip install flask
python receiver_from_vila_with_image.py
# The service listens on http://0.0.0.0:5000/ingest
```

Every new image and its description JSON will appear in the local `./ingested` folder.



